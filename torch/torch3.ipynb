{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치 제공 데이터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as tr # 데이터 불러오면서 바로 전처리\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = tr.Compose([tr.Resize(8),tr.ToTensor()]) # (8,8)로 resize, tensor데이터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n 99%|█████████▉| 169336832/170498071 [00:53<00:00, 1584889.64it/s]Extracting ./data\\cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n"
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data',train=True, download=True, transform=transf) #transform=전처리\n",
    "testset = torchvision.datasets.CIFAR10(root='./data',train=False, download=True,transform=transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3, 8, 8])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "trainset[0][0].size() # 이미지: 레이블(튜플형태로 들어가 있음)\n",
    "# (3개 채널(R, G, B), 사이즈)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader이용해서 배치 사이즈로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=50, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=50, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1000"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "len(trainloader) #CIFAR10 train data개수가 50,000개를 batch_size=50으로 나눈 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataiter, next ===> data하나씩 불러올 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data하나씩 불러올때\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([50, 3, 8, 8])"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "images.size() # 배치사이즈, 채널수, 이미지 너비, 이미지 높이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 폴더 내 데이터 불러올 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: './class'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6860cdd22355>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#./class/tiger   ./class/lion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtransf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./class'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# torchvision.datasets.ImageFolder이용하면 클래스 안에 있는 image들을 알아서 search해줌. 각각의 다른 폴더에 대해서 레이블을 다르게 자동으로 매겨줌. 바로 전처리도 이용 가능(transform이용해서)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    206\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m     92\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[0;32m     93\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[1;34m(self, dir)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mNo\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \"\"\"\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: './class'"
     ]
    }
   ],
   "source": [
    "# 같은 클래스별 폴더 이미지 데이터 이용\n",
    "#./class/tiger   ./class/lion\n",
    "transf = tr.Compose([tr.Resize(8), tr.ToTensor()])\n",
    "trainset = torchvision.datasets.ImageFolder(root='./class',transform=transf) \n",
    "# torchvision.datasets.ImageFolder이용하면 클래스 안에 있는 image들을 알아서 search해줌. 각각의 다른 폴더에 대해서 레이블을 다르게 자동으로 매겨줌. 바로 전처리도 이용 가능(transform이용해서)\n",
    "trainloader = DataLoader(trainset, batch_size=1, shuffle=False, num_workers=2)\n",
    "print(len(trainloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 직접 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(20, 32, 32, 3) (20, 1)\n"
    }
   ],
   "source": [
    "train_images = np.random.randint(256,size=(20,32,32,3))\n",
    "train_labels = np.random.randint(2,size=(20,1))\n",
    "\n",
    "print(train_images.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이 양식은 꼭 기억해놓을 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorData(Dataset): # 상속받을 클래스\n",
    "    def __init__(self, x_data,y_data): # 외부에서 들어올 데이터\n",
    "        self.x_data = torch.FloatTensor(x_data) # tensor로 변환   # 이미지 개수, 채널 수, 이미지 너비, 높이(원래는 이미지 개수, 이미지 너비, 높이, 채널 수)\n",
    "        self.x_data  = self.x_data.permute(0,3,1,2)\n",
    "        self.y_data = torch.LongTensor(y_data) # Float, Long이냐 정할 수 있다.\n",
    "        self.len = self.y_data.shape[0] # 데이터 개수\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index],self.y_data[index] # x,y를 튜플 형태로 밖으로 내보냄\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorData(train_images,train_labels)\n",
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3, 32, 32])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "train_data[0][0].size() # 튜플 형태이기 때문에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([10, 3, 32, 32])"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치로 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,x_data, y_data, transform=None): \n",
    "        self.x_data = x_data  # 텐서로 변환하지 않음\n",
    "        self.y_data = y_data\n",
    "        self.transform = transform\n",
    "        self.len = len(y_data)\n",
    "    def __getitem__(self,index):\n",
    "        sample = self.x_data[index], self.y_data[index]\n",
    "\n",
    "        if self.transform:                 # 튜플 형태로 내보내기 전에 전처리\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class ToTensor:  # 텐서변환\n",
    "    def __call__(self,sample):\n",
    "        inputs, labels = sample\n",
    "        inputs = torch.FloatTensor(inputs)\n",
    "        inputs = inputs.permute(2,0,1)\n",
    "        return inputs, torch.LongTensor(labels)\n",
    "\n",
    "class LinearTensor: # linear형태로 변환\n",
    "    def __init__(self, slope=1, bias=0):\n",
    "        self.slope = slope\n",
    "        self.bias = bias\n",
    "    \n",
    "    def __call__(self,sample):\n",
    "        inputs, labels = sample\n",
    "        inputs = self.slope*inputs + self.bias\n",
    "        return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = tr.Compose([ToTensor(), LinearTensor(2,5)]) # TonTensor왜 만드냐...PIL images여만 사용가능하기 때문데(tr.ToTensor()이용하면)\n",
    "ds1 = MyDataset(train_images, train_labels, transform=trans) # 튜플형태\n",
    "train_loader1 = DataLoader(ds1, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
    }
   ],
   "source": [
    "first_data = ds1[0]\n",
    "features, labels = first_data\n",
    "print(type(features),type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter1 = iter(train_loader1)\n",
    "images1, labels1 = dataiter1.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[[155.,  23., 333.,  ..., 247., 493.,  59.],\n          [169., 211.,  49.,  ..., 437., 437.,  91.],\n          [345.,  79., 441.,  ...,  11., 289.,  11.],\n          ...,\n          [445., 487., 427.,  ..., 505., 465., 383.],\n          [ 15., 397., 167.,  ..., 237., 275., 397.],\n          [449., 201., 277.,  ..., 437., 477., 133.]],\n\n         [[ 11., 429., 291.,  ...,  41., 263., 195.],\n          [445., 199., 219.,  ..., 165., 235., 117.],\n          [ 19., 167., 323.,  ..., 131.,  61., 329.],\n          ...,\n          [117., 277., 417.,  ..., 371., 411., 127.],\n          [385.,  83., 351.,  ...,  49., 385., 251.],\n          [ 31., 513.,  89.,  ..., 305., 145.,  37.]],\n\n         [[ 13., 315.,  79.,  ..., 307., 239., 233.],\n          [427., 477.,   5.,  ..., 107., 417., 155.],\n          [395., 351.,  21.,  ...,  45., 375.,  55.],\n          ...,\n          [353., 177., 277.,  ..., 203., 141., 411.],\n          [485., 241., 387.,  ...,  73.,  43., 485.],\n          [235., 159., 433.,  ..., 513., 445., 271.]]],\n\n\n        [[[387., 237., 137.,  ..., 331., 167., 161.],\n          [ 95., 211., 257.,  ..., 433.,  97., 371.],\n          [361., 415., 477.,  ..., 157.,  53., 361.],\n          ...,\n          [ 83., 299., 411.,  ..., 467., 365., 281.],\n          [289., 431.,  79.,  ...,  55., 101.,  19.],\n          [ 47., 137.,  45.,  ..., 375., 253., 415.]],\n\n         [[421., 355., 291.,  ..., 209.,  61., 139.],\n          [255.,  27., 289.,  ..., 361., 507., 257.],\n          [135., 455., 495.,  ...,  77., 111., 481.],\n          ...,\n          [251.,  19., 237.,  ..., 205., 505., 403.],\n          [479., 391., 121.,  ..., 443.,  53.,  63.],\n          [ 69., 449., 305.,  ..., 339., 103.,  63.]],\n\n         [[511., 177., 295.,  ..., 261., 511., 147.],\n          [297., 501., 183.,  ...,  89., 153., 249.],\n          [139., 173., 219.,  ..., 241., 507., 187.],\n          ...,\n          [221., 147., 299.,  ...,  23., 203., 451.],\n          [503., 155., 449.,  ..., 211., 235., 429.],\n          [ 59.,  53., 365.,  ..., 305., 511., 395.]]],\n\n\n        [[[ 63., 475., 497.,  ..., 209., 211., 509.],\n          [369., 433., 447.,  ..., 427., 427., 385.],\n          [ 95., 269., 269.,  ..., 215., 319., 349.],\n          ...,\n          [131., 445., 471.,  ..., 341.,  49., 419.],\n          [269., 307., 145.,  ..., 449., 515., 373.],\n          [ 89., 135., 467.,  ...,  51., 305., 433.]],\n\n         [[391., 513., 121.,  ..., 427.,  45., 501.],\n          [163., 373., 173.,  ..., 313., 241., 145.],\n          [257., 389.,   5.,  ..., 401., 153., 135.],\n          ...,\n          [257., 361.,  75.,  ..., 491., 473.,  17.],\n          [289., 363., 149.,  ...,  77., 441., 103.],\n          [ 41., 155., 243.,  ..., 109., 495.,  11.]],\n\n         [[275., 495., 115.,  ..., 451., 395., 215.],\n          [303.,  59., 489.,  ..., 419., 277., 205.],\n          [137., 227., 125.,  ..., 251., 405., 225.],\n          ...,\n          [273., 503.,  13.,  ...,  39., 201., 101.],\n          [131., 283.,  27.,  ..., 429., 339., 231.],\n          [353.,  95., 407.,  ..., 377.,   5., 463.]]],\n\n\n        ...,\n\n\n        [[[351., 297., 117.,  ..., 429.,  75.,  67.],\n          [145.,  67., 339.,  ..., 401., 129., 507.],\n          [ 89., 149., 183.,  ...,  57., 305., 379.],\n          ...,\n          [199., 181., 145.,  ..., 351., 269., 357.],\n          [ 43.,  83., 197.,  ..., 181.,  41.,  39.],\n          [231.,  35.,   9.,  ..., 247., 351., 503.]],\n\n         [[115., 103., 379.,  ..., 115., 511.,  51.],\n          [481., 491., 445.,  ..., 433., 483., 223.],\n          [445., 189., 143.,  ..., 101., 257., 301.],\n          ...,\n          [301., 361., 289.,  ..., 219., 425., 515.],\n          [305.,   7., 339.,  ..., 173., 101., 469.],\n          [427., 489., 197.,  ..., 123.,  45., 291.]],\n\n         [[217.,  37.,  67.,  ..., 187., 323.,  17.],\n          [345., 221., 135.,  ..., 179., 443.,  61.],\n          [ 17.,  43., 395.,  ..., 279.,  63.,  41.],\n          ...,\n          [  9., 231., 133.,  ..., 153.,  21., 269.],\n          [115., 443., 307.,  ..., 453., 241., 371.],\n          [ 85., 129., 197.,  ..., 491.,  95.,  27.]]],\n\n\n        [[[305., 347.,  71.,  ..., 231., 117., 373.],\n          [293., 257., 177.,  ..., 297., 319., 405.],\n          [265., 159., 515.,  ..., 403., 485., 369.],\n          ...,\n          [ 99., 457., 271.,  ..., 337., 487., 433.],\n          [291., 401., 483.,  ..., 169., 265., 369.],\n          [ 47., 369., 125.,  ...,  71., 303.,  33.]],\n\n         [[161., 385., 305.,  ..., 245., 337., 395.],\n          [387.,  89., 473.,  ..., 379., 491., 287.],\n          [247.,  71., 401.,  ...,  85., 287., 503.],\n          ...,\n          [409., 419.,  97.,  ..., 317., 449., 405.],\n          [173., 157.,   5.,  ..., 173., 141., 419.],\n          [465., 247., 247.,  ..., 185., 469., 319.]],\n\n         [[513., 429., 175.,  ..., 289., 303., 405.],\n          [171., 355., 199.,  ...,  63., 259., 463.],\n          [ 35., 369., 509.,  ...,  17., 103., 367.],\n          ...,\n          [131.,  17.,  43.,  ..., 285., 443.,  55.],\n          [179., 189., 185.,  ...,  53., 495.,  53.],\n          [175., 269., 133.,  ..., 341., 319., 377.]]],\n\n\n        [[[ 31., 435., 489.,  ..., 399., 407., 457.],\n          [ 23., 241., 355.,  ...,  45., 427., 149.],\n          [183., 269., 309.,  ..., 145.,  35., 399.],\n          ...,\n          [109.,  57., 493.,  ...,  93., 497., 225.],\n          [337., 265., 125.,  ..., 317.,  17., 293.],\n          [373., 495., 497.,  ..., 313., 415., 421.]],\n\n         [[ 99.,  71., 195.,  ..., 277.,  73., 197.],\n          [515., 459.,  87.,  ..., 499., 155., 183.],\n          [359., 371., 455.,  ..., 251., 513., 127.],\n          ...,\n          [411.,  21., 111.,  ..., 437., 435., 309.],\n          [423., 433., 189.,  ..., 381., 345., 367.],\n          [513., 299., 127.,  ..., 165., 297., 271.]],\n\n         [[187., 451., 337.,  ..., 305., 159., 227.],\n          [  7., 119., 359.,  ..., 207., 373., 509.],\n          [413., 239., 329.,  ..., 159., 171., 465.],\n          ...,\n          [115.,  29., 411.,  ..., 285., 283.,  61.],\n          [ 69., 367., 403.,  ..., 463., 271., 403.],\n          [233., 185., 141.,  ..., 129., 149., 469.]]]])"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "images1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor---->PIL image먼저 변환 후 tr.ToTensor()사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,x_data, y_data, transform=None): \n",
    "        self.x_data = x_data  # 텐서로 변환하지 않음\n",
    "        self.y_data = y_data\n",
    "        self.transform = transform\n",
    "        self.len = len(y_data)\n",
    "    def __getitem__(self,index):\n",
    "        sample = self.x_data[index], self.y_data[index]\n",
    "\n",
    "        if self.transform:                 # 튜플 형태로 내보내기 전에 전처리\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class MyTransform:\n",
    "    def __call__(self, sample):\n",
    "        inputs, labels = sample\n",
    "        inputs = torch.FloatTensor(inputs)\n",
    "        inputs = inputs.permute(2,0,1)\n",
    "        labels = torch.FloatTensor(labels)\n",
    "\n",
    "        transf = tr.Compose([tr.ToPILImage(),tr.Resize(128),tr.ToTensor(), tr.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "        final_output = transf(inputs)\n",
    "\n",
    "        return final_output, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = MyDataset(train_images, train_labels, transform = MyTransform())\n",
    "train_loader2 = DataLoader(ds2, batch_size=10, shuffle=True) # batch형태 쓰는 이유는 전체 데이터 쓰면 너무 느리기 때문에 일부만 넣어서 학습햐려고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data=ds2[0]\n",
    "features, labels = first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter2 = iter(train_loader2)\n",
    "images2, labels2 = dataiter2.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([10, 3, 128, 128])"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "images2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([10, 1])"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "labels2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}