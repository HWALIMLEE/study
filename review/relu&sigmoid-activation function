Sigmoid 함수는 binary classification 에 적절함 함수다. 

일정 값을 기준으로 0인지 1인지구분함으로써 분류하는 방식이다. 

딥러닝에서는 특정 임계치를 넘을 때만 활성화되기 때문에 activation function 중의 하나로 구분되는 함수다.

보통 처음은 input layer, 마지막은 output layer 라고 하는데, 이 가운데 보이지 않는 부분은 hidden layer로 칭한다. 

실제로 9개의 hidden layer 가 있다고 했을 때 Tensorflow에 돌려 보면 정확도가 0.5밖에 안되는.. 1개 hidden layer일때 보다도 못한 결과가 나오게 된다. 왜 이런 문제가 발생할까?

>>backpropagation(1986): 2단 3단 정도의 레이어는 학습이 잘 되나, 9단, 10단으로 넘어가면서 부터는 학습이 제대로 이루어 지지 않는 이유는, 역전파 방식에 있다고 볼 수 있다. 
  레이어가 많을 경우 각각의 단계의 값을 미분해서 최초 레이어까지 결과 값을 전달해가게 되는데, 
  만약 내부의 hidden layer들이 모두 sigmoid 함수로 이루어져 있다면 각 단계에서 계산한 값은 모두 0과 1 사이의 값일 수밖에 없다.

>>Vanishing Gradient: 따라서 여러 레이어를 갖고 있을 때, 최초 입력 값은 각각의 레이어에서 나온 값들을 곱해준 만큼의 결과에 영향을 주는 것이므로 
  최종 미분값은 결국 0에 가까운 값이 될 수 밖에 없다. 이를 경사도(기울기)가 사라지는 현상으로 본다. 최초 입력 값이 최종 결과 값에 별로 영향을 끼치지 않는다는 결론으로 수렴하게 되는 것이다.

**sigmoid 함수는 0<n<1 사이의 값만 다루므로 결국 chain rule을 이용해 계속 값을 곱해나간다고 했을 때 결과 값이 0에 수렴할 수 밖에 없다는 한계를 가지고 있으므로, 
나중에는 1보다 작아지지 않게 하기 위한 대안으로 ReLU라는 함수를 적용하게 된다.

<ReLU>
이후 내부 hidden layer를 활성화 시키는 함수로 sigmoid를 사용하지 않고 ReLU라는 활성화 함수를 사용하게 되는데, 
이 함수는 쉽게 말해 0보다 작은 값이 나온 경우 0을 반환하고, 0보다 큰 값이 나온 경우 그 값을 그대로 반환하는 함수다.
 0보다 큰 값일 경우 1을 반환하는 sigmoid와 다르다. 따라서 내부 hidden layer에는 ReLU를 적용하고, 마지막 output layer에서만 sigmoid 함수를 적용하면 이전에 비해 정확도가 훨씬 올라가게 된다.